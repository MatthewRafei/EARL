module Main

import "std/io.earl"
import "std/basic-lexer.earl"

enum MainState {
    None      = 0,
    Functions = 1 << 0,
    Classes   = 1 << 1,
    Variables = 1 << 2,
    Enums     = 1 << 3,
    Methods   = 1 << 4,
}

enum SubState {
    None        = 0,
    Module      = 1 << 0,
    Parameter   = 1 << 1,
    Name        = 1 << 2,
    Returns     = 1 << 3,
    Description = 1 << 4,
}

class State [] {
    @pub let main_state, sub_state = (MainState.None, SubState.None);

    @pub fn remove_main(s) {
        this.main_state `&= `~s;
    }

    @pub fn add_main(s) {
        this.main_state `|= s;
    }

    @pub fn remove_sub() {
        this.sub_state = SubState.None;
    }

    @pub fn add_sub(s) {
        this.sub_state = s;
    }
}

enum Keyword {
    Begin       = "BEGIN",
    End         = "END",
    Module      = "MODULE",
    Name        = "NAME",
    Parameter   = "PARAMETER",
    Returns     = "RETURNS",
    Description = "DESCRIPTION",
    Classes     = "CLASSES",
    Enums       = "ENUMS",
    Methods     = "METHODS",
    Functions   = "FUNCTIONS",
    Variables   = "VARIABLES",
}

fn expect(lexer, ty) {
    let t = lexer.next();
    if !t {
        panic("out of tokens");
    }
    let tx = t.unwrap();
    if tx.ty != ty {
        panic("expected type ", ty, " but got ", tx.ty);
    }
    return tx;
}

fn parse(@const @ref lexer) {
    let cur = lexer.next();
    let state = State();
    while cur {
        if lexer.sz() >= 4
            && cur.unwrap().ty == BasicLexer::TokenType.Hash
            && lexer.peek(0).unwrap().ty == BasicLexer::TokenType.Hash
            && lexer.peek(1).unwrap().ty == BasicLexer::TokenType.Hash
        {
            let _, _, kw = (lexer.next(), lexer.next(), lexer.next().unwrap());
            if kw.ty != BasicLexer::TokenType.Keyword {
                match kw.lx {
                    Keyword.Module -> { state.add_sub(SubState.Module); }
                    Keyword.Name -> { state.add_sub(SubState.Name); }
                    Keyword.Parameter -> { state.add_sub(SubState.Parameter); }
                    Keyword.Returns -> { state.add_sub(SubState.Returns); }
                    Keyword.Description -> { state.add_sub(SubState.Description); }
                }
            }
            else if kw.lx == Keyword.Begin {
                let section = expect(lexer, BasicLexer::TokenType.Keyword);
                match section.lx {
                    Keyword.Functions -> { state.add_main(MainState.Functions); }
                    Keyword.Classes -> { state.add_main(MainState.Classes); }
                    Keyword.Variables -> { state.add_main(MainState.Variables); }
                    Keyword.Enums -> { state.add_main(MainState.Enums); }
                    _ -> { panic("unknown " + Keyword.Begin + " section: " + section.lx); }
                }
            }
            else if kw.lx == Keyword.End {
                let section = expect(lexer, BasicLexer::TokenType.Keyword);
                match section.lx {
                    Keyword.Functions -> { state.remove_main(MainState.Functions); }
                    Keyword.Classes -> { state.remove_main(MainState.Classes); }
                    Keyword.Variables -> { state.remove_main(MainState.Variables); }
                    Keyword.Enums -> { state.remove_main(MainState.Enums); }
                    _ -> { panic("unknown " + Keyword.End + " section: " + section.lx); }
                }
            }
            else {
            }
        }
        cur = lexer.next();
    }
}

fn main() {
    let fp = "./std/os.earl";
    let src = IO::file_to_str(fp);

    let keywords = (
        Keyword.Begin,
        Keyword.Module,
        Keyword.Name,
        Keyword.Parameter,
        Keyword.Returns,
        Keyword.Description,
        Keyword.Classes,
        Keyword.Enums,
        Keyword.Methods,
        Keyword.Functions,
        Keyword.Variables,
        Keyword.End
    );
    let lexer = BasicLexer::T(keywords);

    lexer.lex(src, fp);
    parse(lexer);
}

main();
